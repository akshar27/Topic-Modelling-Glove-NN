{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 1427922,
          "sourceType": "datasetVersion",
          "datasetId": 836238
        }
      ],
      "dockerImageVersionId": 30823,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "NN Glove Topic modeling",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akshar27/Topic-Modelling-Glove-NN/blob/main/NN_Glove_Topic_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "blessondensil294_topic_modeling_for_research_articles_path = kagglehub.dataset_download('blessondensil294/topic-modeling-for-research-articles')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "ECf3Ehme9Fvj"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq glove-python3"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-21T10:28:04.83493Z",
          "iopub.execute_input": "2025-01-21T10:28:04.835232Z",
          "iopub.status.idle": "2025-01-21T10:28:24.694642Z",
          "shell.execute_reply.started": "2025-01-21T10:28:04.835208Z",
          "shell.execute_reply": "2025-01-21T10:28:24.693598Z"
        },
        "id": "ZDIXUIDW9Fvk",
        "outputId": "3e6f969d-b7b6-4199-b73a-3285e3446691"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.0/327.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for glove-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from glove import Corpus, Glove\n",
        "from nltk.tokenize import word_tokenize\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, Embedding, Flatten, LSTM\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the train dataset\n",
        "train_data_path = '/kaggle/input/topic-modeling-for-research-articles/train.csv'  # Update with your local file path\n",
        "train_df = pd.read_csv(train_data_path)\n",
        "\n",
        "# Load the test dataset\n",
        "test_data_path = '/kaggle/input/topic-modeling-for-research-articles/test.csv'  # Update with your local file path\n",
        "test_df = pd.read_csv(test_data_path)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-21T10:28:24.695997Z",
          "iopub.execute_input": "2025-01-21T10:28:24.696255Z",
          "iopub.status.idle": "2025-01-21T10:28:34.374673Z",
          "shell.execute_reply.started": "2025-01-21T10:28:24.696235Z",
          "shell.execute_reply": "2025-01-21T10:28:34.373995Z"
        },
        "id": "MIlqBEMP9Fvk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-21T10:28:34.376587Z",
          "iopub.execute_input": "2025-01-21T10:28:34.376911Z",
          "iopub.status.idle": "2025-01-21T10:28:34.508617Z",
          "shell.execute_reply.started": "2025-01-21T10:28:34.376888Z",
          "shell.execute_reply": "2025-01-21T10:28:34.507782Z"
        },
        "id": "C7NHpYAd9Fvk",
        "outputId": "c8605588-bc89-43bc-e124-a190e665cfea"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n",
          "output_type": "stream"
        },
        {
          "execution_count": 5,
          "output_type": "execute_result",
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize the abstract\n",
        "tokenized_abstract = [word_tokenize(msg.lower()) for msg in train_df['ABSTRACT']]\n",
        "\n",
        "# Create glove corpus and generate a co-occurance matrix\n",
        "corpus = Corpus()\n",
        "corpus.fit(tokenized_abstract, window=4)\n",
        "\n",
        "#Train glove model\n",
        "glove = Glove(no_components=50, learning_rate = 0.05)\n",
        "glove.fit(corpus.matrix, epochs=50, no_threads=2, verbose=True)\n",
        "glove.add_dictionary(corpus.dictionary)\n",
        "\n",
        "# create a word embedding matrix\n",
        "vocab_size = len(glove.dictionary)\n",
        "embedding_dim = glove.no_components\n",
        "\n",
        "# Map word to Glove Embedding\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, idx in glove.dictionary.items():\n",
        "    embedding_matrix[idx] = glove.word_vectors[idx]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-21T10:28:34.509861Z",
          "iopub.execute_input": "2025-01-21T10:28:34.510095Z",
          "iopub.status.idle": "2025-01-21T10:30:32.008884Z",
          "shell.execute_reply.started": "2025-01-21T10:28:34.510074Z",
          "shell.execute_reply": "2025-01-21T10:30:32.008196Z"
        },
        "id": "WKkau1We9Fvk",
        "outputId": "36a3dbce-27fe-40e9-f481-393e86c6c800"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Performing 50 training epochs with 2 threads\nEpoch 0\nEpoch 1\nEpoch 2\nEpoch 3\nEpoch 4\nEpoch 5\nEpoch 6\nEpoch 7\nEpoch 8\nEpoch 9\nEpoch 10\nEpoch 11\nEpoch 12\nEpoch 13\nEpoch 14\nEpoch 15\nEpoch 16\nEpoch 17\nEpoch 18\nEpoch 19\nEpoch 20\nEpoch 21\nEpoch 22\nEpoch 23\nEpoch 24\nEpoch 25\nEpoch 26\nEpoch 27\nEpoch 28\nEpoch 29\nEpoch 30\nEpoch 31\nEpoch 32\nEpoch 33\nEpoch 34\nEpoch 35\nEpoch 36\nEpoch 37\nEpoch 38\nEpoch 39\nEpoch 40\nEpoch 41\nEpoch 42\nEpoch 43\nEpoch 44\nEpoch 45\nEpoch 46\nEpoch 47\nEpoch 48\nEpoch 49\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # Preprocess the text data\n",
        "# def preprocess_text(text):\n",
        "#     text = text.lower()\n",
        "#     text = ''.join(c for c in text if c.isalnum() or c.isspace())\n",
        "#     return text\n",
        "\n",
        "# # Apply preprocessing\n",
        "# train_df['cleaned_text'] = train_df['ABSTRACT'].apply(preprocess_text)\n",
        "\n",
        "# # Convert text to a bag-of-words representation\n",
        "# vectorizer = CountVectorizer(max_features=1000, stop_words='english')\n",
        "# X = vectorizer.fit_transform(train_df['cleaned_text']).toarray()\n",
        "\n",
        "# Encode the target labels\n",
        "Y = train_df[['Computer Science', 'Physics', 'Mathematics', 'Statistics',\n",
        "        'Quantitative Biology', 'Quantitative Finance']]  # Replace 'LABEL' with the actual column name in your dataset"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-21T10:31:38.83706Z",
          "iopub.execute_input": "2025-01-21T10:31:38.837364Z",
          "iopub.status.idle": "2025-01-21T10:31:38.888464Z",
          "shell.execute_reply.started": "2025-01-21T10:31:38.837339Z",
          "shell.execute_reply": "2025-01-21T10:31:38.887391Z"
        },
        "id": "w-Dxz72D9Fvk",
        "outputId": "1a192c44-8108-4b61-b203-3f77b6b36885"
      },
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3790\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3791\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3792\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 1",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-06e645a3f3f4>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         'Quantitative Biology', 'Quantitative Finance']]  # Replace 'LABEL' with the actual column name in your dataset\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3891\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3892\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3893\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3894\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3895\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3796\u001b[0m             ):\n\u001b[1;32m   3797\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3798\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3799\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3800\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 1"
          ],
          "ename": "KeyError",
          "evalue": "1",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#convert sentences to sequences of indices\n",
        "def encode_message(msg, dictionary, max_length=10):\n",
        "    tokens = word_tokenize(msg.lower())\n",
        "    return [dictionary.get(word, 0) for word in tokens][:max_length]\n",
        "\n",
        "# define maximum message length\n",
        "max_length = 100\n",
        "encoded_messages = [encode_message(msg, glove.dictionary, max_length) for msg in train_df['ABSTRACT']]\n",
        "\n",
        "#Pad sequences to uniform length\n",
        "X = np.array([seq + [0]*(max_length-len(seq)) for seq in encoded_messages])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-21T10:30:32.342094Z",
          "iopub.status.idle": "2025-01-21T10:30:32.342371Z",
          "shell.execute_reply": "2025-01-21T10:30:32.342261Z"
        },
        "id": "TaVc6ui79Fvk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-21T10:30:32.343294Z",
          "iopub.status.idle": "2025-01-21T10:30:32.343639Z",
          "shell.execute_reply": "2025-01-21T10:30:32.343482Z"
        },
        "id": "9T9v865w9Fvk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Define model\n",
        "# model = Sequential([\n",
        "#     Input(shape=(X.shape[1],)),  # Input layer\n",
        "#     Dense(256, activation='relu'),          # Hidden layer 1\n",
        "#     Dropout(0.3),                           # Dropout for regularization\n",
        "#     Dense(128, activation='relu'),          # Hidden layer 2\n",
        "#     Dropout(0.3),\n",
        "#     Dense(Y_one_hot.shape[1], activation='sigmoid' if Y_one_hot.ndim > 1 else 'softmax')  # Output layer\n",
        "# ])\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim = vocab_size, output_dim=embedding_dim,\n",
        "             weights =[embedding_matrix] , input_length = max_length, trainable=False),\n",
        "    Flatten(),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(Y.shape[1], activation='sigmoid' if Y.ndim > 1 else 'softmax')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='binary_crossentropy' if Y.ndim > 1 else 'categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-21T10:30:32.344992Z",
          "iopub.status.idle": "2025-01-21T10:30:32.345285Z",
          "shell.execute_reply": "2025-01-21T10:30:32.345172Z"
        },
        "id": "gFMxgEPQ9Fvk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, Y_train,\n",
        "    validation_data=(X_val, Y_val),\n",
        "    epochs=50,        # Adjust number of epochs\n",
        "    batch_size=16,    # Adjust batch size\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-21T10:30:32.346201Z",
          "iopub.status.idle": "2025-01-21T10:30:32.346511Z",
          "shell.execute_reply": "2025-01-21T10:30:32.346403Z"
        },
        "id": "dYv7kZDi9Fvl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on test data\n",
        "loss, accuracy = model.evaluate(X_val, Y_val, verbose=0)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-21T10:30:32.347597Z",
          "iopub.status.idle": "2025-01-21T10:30:32.347924Z",
          "shell.execute_reply": "2025-01-21T10:30:32.347818Z"
        },
        "id": "jDjLrUq-9Fvl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot accuracy\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Accuracy')\n",
        "plt.show()\n",
        "\n",
        "# Plot loss\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-21T10:30:32.350165Z",
          "iopub.status.idle": "2025-01-21T10:30:32.350512Z",
          "shell.execute_reply": "2025-01-21T10:30:32.350322Z"
        },
        "id": "orzIMZuO9Fvl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Example input text\n",
        "new_text = [test_df['ABSTRACT'][10]]\n",
        "\n",
        "# Use the same TF-IDF vectorizer as used during training\n",
        "# new_texts_transformed = vectorizer.transform(new_text).toarray()\n",
        "\n",
        "max_length = 100\n",
        "encoded_messages = [encode_message(msg, glove.dictionary, max_length) for msg in new_text]\n",
        "\n",
        "#Pad sequences to uniform length\n",
        "X = np.array([seq + [0]*(max_length-len(seq)) for seq in encoded_messages])\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-21T10:30:32.351473Z",
          "iopub.status.idle": "2025-01-21T10:30:32.351754Z",
          "shell.execute_reply": "2025-01-21T10:30:32.35162Z"
        },
        "id": "V_qnqLhH9Fvl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict class probabilities\n",
        "class_labels = ['Computer Science', 'Physics', 'Mathematics', 'Statistics',\n",
        "        'Quantitative Biology', 'Quantitative Finance']  # Replace with your actual class labels\n",
        "\n",
        "# Fit the LabelEncoder with class labels\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(class_labels)\n",
        "\n",
        "predictions = model.predict(X)\n",
        "\n",
        "# Get the predicted class indices\n",
        "predicted_class_indices = predictions.argmax(axis=1)\n",
        "\n",
        "# Convert indices back to class labels\n",
        "predicted_classes = label_encoder.inverse_transform(predicted_class_indices)\n",
        "\n",
        "# Display predictions\n",
        "for text, pred in zip(new_text, predicted_classes):\n",
        "    print(f\"Text: {text}\\nPredicted Class: {pred}\\n\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-21T10:30:32.353065Z",
          "iopub.status.idle": "2025-01-21T10:30:32.353381Z",
          "shell.execute_reply": "2025-01-21T10:30:32.353275Z"
        },
        "id": "O8PkJJvv9Fvl"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}